{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "\n",
    "# $\\textbf{Merge Files}$\n",
    "\n",
    "<center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $\\textbf{Code}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--conf \"spark.executor.extraJavaOptions=-XX:ReservedCodeCacheSize=2048m\" --conf \"spark.driver.extraJavaOptions=-XX:ReservedCodeCacheSize=2048m\" pyspark-shell'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('MergeFiles').master(\"local\").enableHiveSupport().getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set paths to the Parquet files\n",
    "file_paths = [\"FilesParquet/Covid.parquet\", \"FilesParquet/GDP.parquet\", \"FilesParquet/Inflation.parquet\",\n",
    "              \"FilesParquet/Migration.parquet\", \"FilesParquet/Population.parquet\", \"FilesParquet/Tax.parquet\", \"FilesParquet/Unemployment.parquet\"]\n",
    "\n",
    "# Read Parquet files into DataFrames\n",
    "dfs = [spark.read.parquet(file_path) for file_path in file_paths]\n",
    "\n",
    "# Merge or outer join the DataFrames\n",
    "merged_df = dfs[0]\n",
    "for i, df in enumerate(dfs):\n",
    "    # Generate new column names with a suffix indicating the iteration number\n",
    "    renamed_columns = [(c, f\"{c}_df{i+1}\") for c in df.columns if c == \"year\"]\n",
    "    # Rename columns in the DataFrame\n",
    "    for old_name, new_name in renamed_columns:\n",
    "        df = df.withColumnRenamed(old_name, new_name)\n",
    "    # Update the DataFrame in the list\n",
    "    dfs[i] = df\n",
    "\n",
    "for df in dfs[1:]:\n",
    "    merged_df = merged_df.join(df, on=\"country\", how=\"outer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Storing this dataframe in parquet\n",
    "merged_df.write.mode(\"overwrite\").parquet(\"FinalOuterParquet\")\n",
    "spark.read.parquet(\"FilesParquet/Covid\").show()\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DataScience",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
