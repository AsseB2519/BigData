{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "\n",
    "# $\\textbf{Processing All in One}$\n",
    "\n",
    "<center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $\\textbf{Code}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType\n",
    "from pyspark.sql.functions import explode, col, lit, array, struct, regexp_replace, year, sum, col, when"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "inicio = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://LAPTOP-CI4QET97.lan:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>AllInOne</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x2b10d5d5950>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName('AllInOne').master(\"local\").enableHiveSupport().getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $\\textbf{Carregar os Datasets}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating dataframes from the csv's and xlsx files and infering the schema\n",
    "df_covid = spark.read.load(\"Files/Covid.csv\", format=\"csv\", sep=\",\", inferschema=\"true\", header=\"true\")\n",
    "df_gdp = spark.read.load(\"Files/GDP.csv\", format=\"csv\", sep=\",\", inferschema=\"true\", header=\"true\")\n",
    "df_migration = spark.read.load(\"Files/Migration.csv\", format=\"csv\", sep=\",\", inferschema=\"true\", header=\"true\")\n",
    "df_population = spark.read.load(\"Files/Population.csv\", format=\"csv\", sep=\",\", inferschema=\"true\", header=\"true\")\n",
    "df_tax = spark.read.load(\"Files/Tax.csv\", format=\"csv\", sep=\",\", inferschema=\"true\", header=\"true\")\n",
    "df_unemployment = spark.read.load(\"Files/Unemployment.csv\", format=\"csv\", sep=\",\", inferschema=\"true\", header=\"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Read the Excel file using pandas\n",
    "pandas_df = pd.read_excel(\"Files/Inflation.xlsx\")\n",
    "\n",
    "# Replace \"no data\" with NaN\n",
    "pandas_df.replace(\"no data\", float(\"nan\"), inplace=True)\n",
    "\n",
    "# Extract column names from the first row\n",
    "column_names = [str(col) for col in pandas_df.columns]\n",
    "\n",
    "# Step 5: Define the schema for the Spark DataFrame\n",
    "schema_fields = [StructField(column_names[0], StringType(), True)] + \\\n",
    "                [StructField(col, DoubleType(), True) for col in column_names[1:]]\n",
    "\n",
    "# Create schema\n",
    "schema = StructType(schema_fields)\n",
    "\n",
    "# Step 6: Convert the pandas DataFrame to a Spark DataFrame with the specified schema\n",
    "df_inflation = spark.createDataFrame(pandas_df, schema=schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $\\textbf{Covid - Processamento}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the columns you're interested in\n",
    "df_covid = df_covid.select(\"location\",\"date\",\"total_cases\")\n",
    "\n",
    "# Rename the columns \"location\" and \"total_cases\"\n",
    "df_covid = df_covid.withColumnRenamed(\"location\",\"country\")\n",
    "df_covid = df_covid.withColumnRenamed(\"total_cases\",\"covid_cases\")\n",
    "\n",
    "# Extract year from 'day' column\n",
    "df_covid = df_covid.withColumn(\"year\", year(\"date\"))\n",
    "df_covid = df_covid.drop(\"date\")\n",
    "\n",
    "# Replace null values in 'total_cases' column with 0\n",
    "df_covid = df_covid.withColumn(\"covid_cases\", when(df_covid[\"covid_cases\"].isNull(), 0).otherwise(df_covid[\"covid_cases\"]))\n",
    "\n",
    "# Remove non-numeric characters from 'total_cases' column\n",
    "df_covid = df_covid.withColumn(\"covid_cases\", regexp_replace(\"covid_cases\", \"[^0-9]\", \"\"))\n",
    "\n",
    "# Cast columns to their desired types\n",
    "df_covid = df_covid.withColumn(\"country\", col(\"country\").cast(\"string\"))\n",
    "df_covid = df_covid.withColumn(\"year\", col(\"year\").cast(\"int\"))\n",
    "df_covid = df_covid.withColumn(\"covid_cases\", col(\"covid_cases\").cast(\"int\"))\n",
    "\n",
    "# Filter out any null values after cleaning\n",
    "df_covid = df_covid.filter(col(\"covid_cases\").isNotNull())\n",
    "\n",
    "# Calculate total cases per year\n",
    "df_covid = df_covid.groupBy(\"country\", \"year\").agg(sum(\"covid_cases\"))\n",
    "\n",
    "# Order by country and then by year\n",
    "df_covid = df_covid.orderBy(\"country\", \"year\")\n",
    "\n",
    "# Rename 'sum(covid_cases)' to 'covid_cases'\n",
    "df_covid = df_covid.withColumnRenamed(\"sum(covid_cases)\",\"covid_cases\")\n",
    "\n",
    "# Rename the column\n",
    "df_covid = df_covid.withColumnRenamed(\"Micronesia (country)\",\"Micronesia\")\n",
    "df_covid = df_covid.withColumnRenamed(\"Saint Martin (French part)\",\"Saint Martin\")\n",
    "df_covid = df_covid.withColumnRenamed(\"Sint Maarten (Dutch part)\",\"Sint Maarten\")\n",
    "df_covid = df_covid.withColumnRenamed(\"Brunei\",\"Brunei Darussalam\")\n",
    "df_covid = df_covid.withColumnRenamed(\"Cape Verde\",\"Cabo Verde\")\n",
    "df_covid = df_covid.withColumnRenamed(\"Laos\",\"Lao PDR\")\n",
    "df_covid = df_covid.withColumnRenamed(\"United States Virgin Islands\",\"Virgin Islands (U.S.)\")\n",
    "df_covid = df_covid.withColumnRenamed(\"Turkey\",\"Turkiye\")\n",
    "df_covid = df_covid.withColumnRenamed(\"Syria\",\"Syrian Arab Republic\")\n",
    "df_covid = df_covid.withColumnRenamed(\"Timor\",\"Timor-Leste\")\n",
    "df_covid = df_covid.withColumnRenamed(\"Russia\",\"Russian Federation\")\n",
    "df_covid = df_covid.withColumnRenamed(\"Congo\",\"Democratic Republic of the Congo\")\n",
    "df_covid = df_covid.withColumnRenamed(\"Slovakia\",\"Slovenia\")\n",
    "\n",
    "# Drop the specified lines\n",
    "countries_to_remove = [\"Africa\", \"Europe\", \"Europe Union\", \"Asia\", \"Lower middle income\", \"Upper middle income\", \"World\"]\n",
    "df_covid = df_covid.filter(~df_covid[\"country\"].isin(countries_to_remove))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $\\textbf{GDP - Processamento}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming df_gdp is properly defined DataFrame\n",
    "df_gdp = df_gdp.select(\"country\", explode(array([\n",
    "    struct(lit(year).alias(\"year\"), col(str(year)).alias(\"gdp\")) \n",
    "    for year in range(2000, 2025)\n",
    "])).alias(\"data\")).selectExpr(\"country\", \"data.year\", \"data.gdp\")\n",
    "\n",
    "# Cast columns to their desired types\n",
    "df_gdp = df_gdp.withColumn(\"country\", col(\"country\").cast(\"string\"))\n",
    "df_gdp = df_gdp.withColumn(\"year\", col(\"year\").cast(\"int\"))\n",
    "df_gdp = df_gdp.withColumn(\"gdp\", col(\"gdp\").cast(\"double\"))\n",
    "\n",
    "# Filter data for years greater than 2010 and lower than 2024\n",
    "df_gdp = df_gdp.filter(df_gdp[\"year\"] > 2010)\n",
    "df_gdp = df_gdp.filter(df_gdp[\"year\"] < 2024)\n",
    "\n",
    "# Order by country and then by year\n",
    "df_gdp = df_gdp.orderBy(\"country\", \"year\")\n",
    "\n",
    "# Rename the column\n",
    "df_gdp = df_gdp.withColumnRenamed(\"China, People\\'s Republic of\",\"China\")\n",
    "df_gdp = df_gdp.withColumnRenamed(\"Gambia, The\",\"Gambia\")\n",
    "df_gdp = df_gdp.withColumnRenamed(\"Micronesia, Fed. States of\",\"Micronesia\")\n",
    "df_gdp = df_gdp.withColumnRenamed(\"South Sudan, Republic of\",\"South Sudan\")\n",
    "df_gdp = df_gdp.withColumnRenamed(\"Taiwan Province of China\",\"Taiwan\")\n",
    "df_gdp = df_gdp.withColumnRenamed(\"Türkiye, Republic of\",\"Turkiye\")\n",
    "df_gdp = df_gdp.withColumnRenamed(\"Bahamas, The\",\"Bahamas\")\n",
    "df_gdp = df_gdp.withColumnRenamed(\"Syria\",\"Syrian Arab Republic\")\n",
    "df_gdp = df_gdp.withColumnRenamed(\"Czech Republic\",\"Czechia\")\n",
    "df_gdp = df_gdp.withColumnRenamed(\"Korea, Republic of\",\"South Korea\")\n",
    "df_gdp = df_gdp.withColumnRenamed(\"Côte d\\'Ivoire\",\"Cote d\\'Ivoire\")\n",
    "df_gdp = df_gdp.withColumnRenamed(\"Hong Kong SAR\",\"Hong Kong\")\n",
    "df_gdp = df_gdp.withColumnRenamed(\"Lao P.D.R.\",\"Lao PDR\")\n",
    "df_gdp = df_gdp.withColumnRenamed(\"Congo, Republic of \",\"Republic of the Congo\")\n",
    "df_gdp = df_gdp.withColumnRenamed(\"Congo, Dem. Rep. of the\",\"Democratic Republic of the Congo\")\n",
    "df_gdp = df_gdp.withColumnRenamed(\"Pacific Islands \",\"Pacific island small states\")\n",
    "df_gdp = df_gdp.withColumnRenamed(\"North Macedonia \",\"North Macedonia\")\n",
    "\n",
    "# Columns to drop\n",
    "countries_to_remove = [\"ASEAN-5\",\"Advanced economies\", \"Africa (Region)\",\"Asia and Pacific\", \"Australia and New Zealand\",\"Central America\",\n",
    "        \"Central Asia and the Caucasus\",\"©IMF, 2023\",\"East Asia\",\"Eastern Europe \",\"Emerging and Developing Asia\",\n",
    "        \"Emerging and Developing Europe\",\"Emerging market and developing economies\",\"Euro area\",\"Europe\",\"European Union\",\n",
    "        \"Latin America and the Caribbean\",\"Macao SAR\",\"Major advanced economies (G7)\",\"Middle East (Region)\",\"Middle East and Central Asia\",\n",
    "        \"North Africa\",\"North America\",\"North Macedonia\",\"Other advanced economies\",\"South America\",\"South Asia\",\"Southeast Asia\",\n",
    "        \"Sub-Saharan Africa\",\"Sub-Saharan Africa (Region)\",\"Western Europe\",\"Western Hemisphere (Region)\",\"World\"]\n",
    "\n",
    "df_gdp = df_gdp.filter(~df_gdp[\"country\"].isin(countries_to_remove))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $\\textbf{Inflation - Processamento}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename the columns \"location\" and \"total_cases\"\n",
    "df_inflation = df_inflation.withColumnRenamed(\"Inflation rate, average consumer prices (Annual percent change)\",\"country\")\n",
    "\n",
    "# Assuming df_inflation is properly defined DataFrame\n",
    "df_inflation = df_inflation.select(\"country\", explode(array([\n",
    "    struct(lit(year).alias(\"year\"), col(str(year)).alias(\"inflation\")) \n",
    "    for year in range(2000, 2025)\n",
    "])).alias(\"data\")).selectExpr(\"country\", \"data.year\", \"data.inflation\")\n",
    "\n",
    "# Cast columns to their desired types\n",
    "df_inflation = df_inflation.withColumn(\"country\", col(\"country\").cast(\"string\"))\n",
    "df_inflation = df_inflation.withColumn(\"year\", col(\"year\").cast(\"int\"))\n",
    "df_inflation = df_inflation.withColumn(\"inflation\", col(\"inflation\").cast(\"double\"))\n",
    "\n",
    "# Filter data for years greater than 2010 and lower than 2024\n",
    "df_inflation = df_inflation.filter(df_inflation[\"year\"] > 2010)\n",
    "df_inflation = df_inflation.filter(df_inflation[\"year\"] < 2024)\n",
    "\n",
    "# Order by country and then by year\n",
    "df_inflation = df_inflation.orderBy(\"country\", \"year\")\n",
    "\n",
    "replacements = {\n",
    "    \"China, People's Republic of\": 'China',\n",
    "    'Gambia, The': 'Gambia',\n",
    "    'Micronesia, Fed. States of': 'Micronesia',\n",
    "    'South Sudan, Republic of': 'South Sudan',\n",
    "    'Taiwan Province of China': 'Taiwan',\n",
    "    'Türkiye, Republic of': 'Turkiye',\n",
    "    'Bahamas, The': 'Bahamas',\n",
    "    'Czech Republic': 'Czechia',\n",
    "    'Syria': 'Syrian Arab Republic',\n",
    "    \"Côte d'Ivoire\": \"Cote d'Ivoire\",\n",
    "    'Hong Kong SAR': 'Hong Kong',\n",
    "    'Lao P.D.R.': 'Lao PDR',\n",
    "    'Korea, Republic of': 'South Korea',\n",
    "    'Congo, Dem. Rep. of the': 'Democratic Republic of the Congo',\n",
    "    'Congo, Republic of ': 'Republic of the Congo',\n",
    "    'Pacific Islands ': 'Pacific island small states',\n",
    "    'North Macedonia ': 'North Macedonia'\n",
    "}\n",
    "\n",
    "# Rename columns based on replacements dictionary\n",
    "for old_value, new_value in replacements.items():\n",
    "    df_inflation = df_inflation.withColumnRenamed(old_value, new_value)\n",
    "\n",
    "# Columns to drop\n",
    "countries_to_remove = ['ASEAN-5', \n",
    "                       'Advanced economies', \n",
    "                       'Africa (Region)', \n",
    "                       'Asia and Pacific', \n",
    "                       'Australia and New Zealand', \n",
    "                       'Central America', \n",
    "                       'Central Asia and the Caucasus', \n",
    "                       '©IMF, 2023',\n",
    "                       'East Asia',\n",
    "                       'Eastern Europe ',\n",
    "                       'Emerging and Developing Asia',\n",
    "                       'Emerging and Developing Europe',\n",
    "                       'Emerging market and developing economies',\n",
    "                       'Euro area',\n",
    "                       'Europe',\n",
    "                       'European Union',\n",
    "                       'Latin America and the Caribbean',\n",
    "                       'Macao SAR',\n",
    "                       'Major advanced economies (G7)',\n",
    "                       'Middle East (Region)',\n",
    "                       'Middle East and Central Asia',\n",
    "                       'North Africa',\n",
    "                       'North America',\n",
    "                       'North Macedonia',\n",
    "                       'Other advanced economies',\n",
    "                       'South America',\n",
    "                       'South Asia',\n",
    "                       'Southeast Asia',\n",
    "                       'Sub-Saharan Africa',\n",
    "                       'Sub-Saharan Africa (Region)',\n",
    "                       'Western Europe',\n",
    "                       'Western Hemisphere (Region)',\n",
    "                       'World',\n",
    "                       'nan']\n",
    "\n",
    "# Drop the specified rows\n",
    "df_inflation = df_inflation.filter(~df_inflation['country'].isin(countries_to_remove))\n",
    "\n",
    "# Remove rows with missing values in the 'country' column\n",
    "df_inflation = df_inflation.dropna(subset=['country'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $\\textbf{Migration - Processamento}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the columns you don't want\n",
    "df_migration = df_migration.drop(\"Series Name\", \"Series Code\", \"Country Code\")\n",
    "\n",
    "# Rename the column\n",
    "df_migration = df_migration.withColumnRenamed(\"Country Name\",\"country\")\n",
    "df_migration = df_migration.withColumnRenamed(\"2000 [YR2000]\",\"2000\")\n",
    "df_migration = df_migration.withColumnRenamed(\"2001 [YR2001]\",\"2001\")\n",
    "df_migration = df_migration.withColumnRenamed(\"2002 [YR2002]\",\"2002\")\n",
    "df_migration = df_migration.withColumnRenamed(\"2003 [YR2003]\",\"2003\")\n",
    "df_migration = df_migration.withColumnRenamed(\"2004 [YR2004]\",\"2004\")\n",
    "df_migration = df_migration.withColumnRenamed(\"2008 [YR2008]\",\"2008\")\n",
    "df_migration = df_migration.withColumnRenamed(\"2009 [YR2009]\",\"2009\")\n",
    "df_migration = df_migration.withColumnRenamed(\"2010 [YR2010]\",\"2010\")\n",
    "df_migration = df_migration.withColumnRenamed(\"2011 [YR2011]\",\"2011\")\n",
    "df_migration = df_migration.withColumnRenamed(\"2012 [YR2012]\",\"2012\")\n",
    "df_migration = df_migration.withColumnRenamed(\"2013 [YR2013]\",\"2013\")\n",
    "df_migration = df_migration.withColumnRenamed(\"2014 [YR2014]\",\"2014\")\n",
    "df_migration = df_migration.withColumnRenamed(\"2015 [YR2015]\",\"2015\")\n",
    "df_migration = df_migration.withColumnRenamed(\"2016 [YR2016]\",\"2016\")\n",
    "df_migration = df_migration.withColumnRenamed(\"2017 [YR2017]\",\"2017\")\n",
    "df_migration = df_migration.withColumnRenamed(\"2018 [YR2018]\",\"2018\")\n",
    "df_migration = df_migration.withColumnRenamed(\"2019 [YR2019]\",\"2019\")\n",
    "df_migration = df_migration.withColumnRenamed(\"2020 [YR2020]\",\"2020\")\n",
    "df_migration = df_migration.withColumnRenamed(\"2021 [YR2021]\",\"2021\")\n",
    "df_migration = df_migration.withColumnRenamed(\"2022 [YR2022]\",\"2022\")\n",
    "df_migration = df_migration.withColumnRenamed(\"2023 [YR2023]\",\"2023\")\n",
    "df_migration = df_migration.withColumnRenamed(\"2024 [YR2024]\",\"2024\")\n",
    "df_migration = df_migration.withColumnRenamed(\"2025 [YR2025]\",\"2025\")\n",
    "\n",
    "# Assuming df_migration is properly defined DataFrame\n",
    "df_migration = df_migration.select(\"country\", explode(array([\n",
    "    struct(lit(year).alias(\"year\"), col(str(year)).alias(\"migration\")) \n",
    "    for year in list(range(2000, 2005)) + list(range(2008, 2025))\n",
    "])).alias(\"data\")).selectExpr(\"country\", \"data.year\", \"data.migration\")\n",
    "\n",
    "# Cast columns to their desired types\n",
    "df_migration = df_migration.withColumn(\"country\", col(\"country\").cast(\"string\"))\n",
    "df_migration = df_migration.withColumn(\"year\", col(\"year\").cast(\"int\"))\n",
    "df_migration = df_migration.withColumn(\"migration\", col(\"migration\").cast(\"int\"))\n",
    "\n",
    "# Filter data for years greater than 2010 and lower than 2024\n",
    "df_migration = df_migration.filter(df_migration[\"year\"] > 2010)\n",
    "df_migration = df_migration.filter(df_migration[\"year\"] < 2024)\n",
    "\n",
    "# Order by country and then by year\n",
    "df_migration = df_migration.orderBy(\"country\", \"year\")\n",
    "\n",
    "replacements = {\n",
    "    \"Bahamas, The\": \"Bahamas\",\n",
    "    \"Egypt, Arab Rep.\": \"Egypt\",\n",
    "    \"Micronesia, Fed. Sts.\": \"Micronesia\",\n",
    "    \"Sint Maarten (Dutch part)\": \"Sint Maarten\",\n",
    "    \"St. Martin (French part)\": \"St. Martin\",\n",
    "    \"Venezuela, RB\": \"Venezuela\",\n",
    "    \"Yemen, Rep.\": \"Yemen\",\n",
    "    \"Caribbean small states\": \"Caribbean\",\n",
    "    \"Gambia, The\": \"Gambia\",\n",
    "    \"Hong Kong SAR, China\": \"Hong Kong\",\n",
    "    \"Iran, Islamic Rep.\": \"Iran\",\n",
    "    \"Macao SAR, China\": \"Macao SAR\",\n",
    "    \"Korea, Rep.\": \"South Korea\",\n",
    "    \"Korea, Dem. People's Rep.\": \"Korea\",\n",
    "    \"Congo, Rep.\": \"Republic of the Congo\",\n",
    "    \"Congo, Dem. Rep.\": \"Democratic Republic of the Congo\",\n",
    "    \"Viet Nam\": \"Vietnam\"\n",
    "}\n",
    "\n",
    "# Rename columns based on replacements dictionary\n",
    "for old_value, new_value in replacements.items():\n",
    "    df_migration = df_migration.withColumnRenamed(old_value, new_value)\n",
    "\n",
    "# Columns to drop\n",
    "countries_to_remove = ['Africa Eastern and Southern', \n",
    "                       'Africa Western and Central', \n",
    "                       'Central Europe and the Baltics', \n",
    "                       'Early-demographic dividend', \n",
    "                       'East Asia & Pacific', \n",
    "                       'East Asia & Pacific (IDA & IBRD countries)', \n",
    "                       'East Asia & Pacific (excluding high income)', \n",
    "                       'Euro area',\n",
    "                       'Europe & Central Asia',\n",
    "                       'Europe & Central Asia (IDA & IBRD countries)',\n",
    "                       'Europe & Central Asia (excluding high income)',\n",
    "                       'European Union',\n",
    "                       'Fragile and conflict affected situations',\n",
    "                       'Heavily indebted poor countries (HIPC)',\n",
    "                       'High income',\n",
    "                       'IBRD only',\n",
    "                       'IDA & IBRD total',\n",
    "                       'IDA blend',\n",
    "                       'IDA only',\n",
    "                       'IDA total',\n",
    "                       'Latin America & the Caribbean (IDA & IBRD countries)',\n",
    "                       'Latin America & Caribbean (excluding high income)',\n",
    "                       'Least developed countries: UN classification',\n",
    "                       'Low & middle income',\n",
    "                       'Low income',\n",
    "                       'Lower middle income',\n",
    "                       'Middle East & North Africa',\n",
    "                       'Middle East & North Africa (IDA & IBRD countries)',\n",
    "                       'Middle East & North Africa (excluding high income)',\n",
    "                       'Middle income',\n",
    "                       'Not classified',\n",
    "                       'OECD members',\n",
    "                       'Other small states',\n",
    "                       'Pacific island small states'\n",
    "                       'Post-demographic dividend',\n",
    "                       'Pre-demographic dividend',\n",
    "                       'Small states',\n",
    "                       'South Asia',\n",
    "                       'South Asia (IDA & IBRD)',\n",
    "                       'Sub-Saharan Africa (IDA & IBRD countries)',\n",
    "                       'Sub-Saharan Africa (excluding high income)',\n",
    "                       'Upper middle income',\n",
    "                       'World'\n",
    "                       ]\n",
    "\n",
    "# Drop the specified rows\n",
    "df_migration = df_migration.filter(~df_migration['country'].isin(countries_to_remove))\n",
    "\n",
    "# Remove rows with missing values in the 'country' column\n",
    "df_migration = df_migration.dropna(subset=['country'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $\\textbf{Population - Processamento}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the columns you don't want\n",
    "df_population = df_population.drop(\"Series Name\", \"Series Code\", \"Country Code\")\n",
    "\n",
    "# Rename the column\n",
    "df_population = df_population.withColumnRenamed(\"Country Name\",\"country\")\n",
    "df_population = df_population.withColumnRenamed(\"2000 [YR2000]\",\"2000\")\n",
    "df_population = df_population.withColumnRenamed(\"2001 [YR2001]\",\"2001\")\n",
    "df_population = df_population.withColumnRenamed(\"2002 [YR2002]\",\"2002\")\n",
    "df_population = df_population.withColumnRenamed(\"2003 [YR2003]\",\"2003\")\n",
    "df_population = df_population.withColumnRenamed(\"2004 [YR2004]\",\"2004\")\n",
    "df_population = df_population.withColumnRenamed(\"2005 [YR2005]\",\"2005\")\n",
    "df_population = df_population.withColumnRenamed(\"2006 [YR2006]\",\"2006\")\n",
    "df_population = df_population.withColumnRenamed(\"2007 [YR2007]\",\"2007\")\n",
    "df_population = df_population.withColumnRenamed(\"2008 [YR2008]\",\"2008\")\n",
    "df_population = df_population.withColumnRenamed(\"2009 [YR2009]\",\"2009\")\n",
    "df_population = df_population.withColumnRenamed(\"2010 [YR2010]\",\"2010\")\n",
    "df_population = df_population.withColumnRenamed(\"2011 [YR2011]\",\"2011\")\n",
    "df_population = df_population.withColumnRenamed(\"2012 [YR2012]\",\"2012\")\n",
    "df_population = df_population.withColumnRenamed(\"2013 [YR2013]\",\"2013\")\n",
    "df_population = df_population.withColumnRenamed(\"2014 [YR2014]\",\"2014\")\n",
    "df_population = df_population.withColumnRenamed(\"2015 [YR2015]\",\"2015\")\n",
    "df_population = df_population.withColumnRenamed(\"2016 [YR2016]\",\"2016\")\n",
    "df_population = df_population.withColumnRenamed(\"2017 [YR2017]\",\"2017\")\n",
    "df_population = df_population.withColumnRenamed(\"2018 [YR2018]\",\"2018\")\n",
    "df_population = df_population.withColumnRenamed(\"2019 [YR2019]\",\"2019\")\n",
    "df_population = df_population.withColumnRenamed(\"2020 [YR2020]\",\"2020\")\n",
    "df_population = df_population.withColumnRenamed(\"2021 [YR2021]\",\"2021\")\n",
    "df_population = df_population.withColumnRenamed(\"2022 [YR2022]\",\"2022\")\n",
    "df_population = df_population.withColumnRenamed(\"2023 [YR2023]\",\"2023\")\n",
    "df_population = df_population.withColumnRenamed(\"2024 [YR2024]\",\"2024\")\n",
    "df_population = df_population.withColumnRenamed(\"2025 [YR2025]\",\"2025\")\n",
    "\n",
    "# Assuming df_population is properly defined DataFrame\n",
    "df_population = df_population.select(\"country\", explode(array([\n",
    "    struct(lit(year).alias(\"year\"), col(str(year)).alias(\"population\")) \n",
    "    for year in range(2000, 2025)\n",
    "])).alias(\"data\")).selectExpr(\"country\", \"data.year\", \"data.population\")\n",
    "\n",
    "# Cast columns to their desired types\n",
    "df_population = df_population.withColumn(\"country\", col(\"country\").cast(\"string\"))\n",
    "df_population = df_population.withColumn(\"year\", col(\"year\").cast(\"int\"))\n",
    "df_population = df_population.withColumn(\"population\", col(\"population\").cast(\"int\"))\n",
    "\n",
    "# Filter data for years greater than 2010 and lower than 2024\n",
    "df_population = df_population.filter(df_population[\"year\"] > 2010)\n",
    "df_population = df_population.filter(df_population[\"year\"] < 2024)\n",
    "\n",
    "# Order by country and then by year\n",
    "df_population = df_population.orderBy(\"country\", \"year\")\n",
    "\n",
    "replacements = {\n",
    "    \"Bahamas, The\": \"Bahamas\",\n",
    "    \"Egypt, Arab Rep.\": \"Egypt\",\n",
    "    \"Micronesia, Fed. Sts.\": \"Micronesia\",\n",
    "    \"Sint Maarten (Dutch part)\": \"Sint Maarten\",\n",
    "    \"St. Martin (French part)\": \"St. Martin\",\n",
    "    \"Venezuela, RB\": \"Venezuela\",\n",
    "    \"Yemen, Rep.\": \"Yemen\",\n",
    "    \"Caribbean small states\": \"Caribbean\",\n",
    "    \"Gambia, The\": \"Gambia\",\n",
    "    \"Hong Kong SAR, China\": \"Hong Kong\",\n",
    "    \"Iran, Islamic Rep.\": \"Iran\",\n",
    "    \"Congo, Rep.\": \"Republic of the Congo\",\n",
    "    \"Macao SAR, China\": \"Macao SAR\",\n",
    "    \"Korea, Rep.\": \"South Korea\",\n",
    "    \"Korea, Dem. People's Rep.\": \"Korea\",\n",
    "    \"Congo, Dem. Rep.\": \"Democratic Republic of the Congo\",\n",
    "    \"Viet Nam\": \"Vietnam\"\n",
    "}\n",
    "\n",
    "# Rename columns based on replacements dictionary\n",
    "for old_value, new_value in replacements.items():\n",
    "    df_population = df_population.withColumnRenamed(old_value, new_value)\n",
    "\n",
    "# Columns to drop\n",
    "countries_to_remove = ['Africa Eastern and Southern', \n",
    "                       'Africa Western and Central', \n",
    "                       'Central Europe and the Baltics', \n",
    "                       'Early-demographic dividend', \n",
    "                       'East Asia & Pacific', \n",
    "                       'East Asia & Pacific (IDA & IBRD countries)', \n",
    "                       'East Asia & Pacific (excluding high income)', \n",
    "                       'Euro area',\n",
    "                       'Europe & Central Asia',\n",
    "                       'Europe & Central Asia (IDA & IBRD countries)',\n",
    "                       'Europe & Central Asia (excluding high income)',\n",
    "                       'European Union',\n",
    "                       'Fragile and conflict affected situations',\n",
    "                       'Heavily indebted poor countries (HIPC)',\n",
    "                       'High income',\n",
    "                       'IBRD only',\n",
    "                       'IDA & IBRD total',\n",
    "                       'IDA blend',\n",
    "                       'IDA only',\n",
    "                       'IDA total',\n",
    "                       'Latin America & the Caribbean (IDA & IBRD countries)',\n",
    "                       'Latin America & Caribbean (excluding high income)',\n",
    "                       'Least developed countries: UN classification',\n",
    "                       'Low & middle income',\n",
    "                       'Low income',\n",
    "                       'Lower middle income',\n",
    "                       'Middle East & North Africa',\n",
    "                       'Middle East & North Africa (IDA & IBRD countries)',\n",
    "                       'Middle East & North Africa (excluding high income)',\n",
    "                       'Middle income',\n",
    "                       'Not classified',\n",
    "                       'OECD members',\n",
    "                       'Other small states',\n",
    "                       'Pacific island small states'\n",
    "                       'Post-demographic dividend',\n",
    "                       'Pre-demographic dividend',\n",
    "                       'Small states',\n",
    "                       'South Asia',\n",
    "                       'South Asia (IDA & IBRD)',\n",
    "                       'Sub-Saharan Africa (IDA & IBRD countries)',\n",
    "                       'Sub-Saharan Africa (excluding high income)',\n",
    "                       'Upper middle income',\n",
    "                       'World'\n",
    "                       ]\n",
    "\n",
    "# Drop the specified rows\n",
    "df_population = df_population.filter(~df_population['country'].isin(countries_to_remove))\n",
    "\n",
    "# Remove rows with missing values in the 'country' column\n",
    "df_population = df_population.dropna(subset=['country'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $\\textbf{Tax - Processamento}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename the column\n",
    "df_tax = df_tax.withColumnRenamed(\"Country Name\",\"country\")\n",
    "columns_to_drop = [str(year) for year in range(1960, 2000)]\n",
    "df_tax = df_tax.drop(\"Country Code\",\"Indicator Name\",\"Indicator Code\",*columns_to_drop,\"_c67\")\n",
    "\n",
    "# Assuming df_tax is properly defined DataFrame\n",
    "df_tax = df_tax.select(\"country\", explode(array([\n",
    "    struct(lit(year).alias(\"year\"), col(str(year)).alias(\"tax\")) \n",
    "    for year in range(2000, 2023)\n",
    "])).alias(\"data\")).selectExpr(\"country\", \"data.year\", \"data.tax\")\n",
    "\n",
    "df_tax = df_tax.orderBy(\"country\")\n",
    "\n",
    "# Replace null values in 'total_cases' column with 0\n",
    "df_tax = df_tax.withColumn(\"tax\", when(df_tax[\"tax\"].isNull(), 0).otherwise(df_tax[\"tax\"]))\n",
    "\n",
    "# Cast columns to their desired types\n",
    "df_tax = df_tax.withColumn(\"country\", col(\"country\").cast(\"string\"))\n",
    "df_tax = df_tax.withColumn(\"year\", col(\"year\").cast(\"int\"))\n",
    "df_tax = df_tax.withColumn(\"tax\", col(\"tax\").cast(\"double\"))\n",
    "\n",
    "# Filter data for years greater than 2010 and lower than 2024\n",
    "df_tax = df_tax.filter(df_tax[\"year\"] > 2010)\n",
    "df_tax = df_tax.filter(df_tax[\"year\"] < 2024)\n",
    "\n",
    "# Order by country and then by year\n",
    "df_tax = df_tax.orderBy(\"country\", \"year\")\n",
    "\n",
    "replacements = {\n",
    "    \"China, People's Republic of\": \"China\",\n",
    "    \"Gambia, The\": \"Gambia\",\n",
    "    \"Micronesia, Fed. States of\": \"Micronesia\",\n",
    "    \"South Sudan, Republic of\": \"South Sudan\",\n",
    "    \"Taiwan Province of China\": \"Taiwan\",\n",
    "    \"Türkiye, Republic of\": \"Türkiye\",\n",
    "    \"Korea, Republic of\": \"South Korea\",\n",
    "    \"Congo\": \"Democratic Republic of the Congo\",\n",
    "    \"Caribbean small states\": \"Caribbean\"\n",
    "}\n",
    "\n",
    "# Rename columns based on replacements dictionary\n",
    "for old_value, new_value in replacements.items():\n",
    "    df_tax = df_tax.withColumnRenamed(old_value, new_value)\n",
    "\n",
    "# Columns to drop\n",
    "countries_to_remove = ['Africa Eastern and Southern', \n",
    "                       'Africa Western and Central', \n",
    "                       'Central Europe and the Baltics', \n",
    "                       'Early-demographic dividend', \n",
    "                       'East Asia & Pacific', \n",
    "                       'East Asia & Pacific (IDA & IBRD countries)', \n",
    "                       'East Asia & Pacific (excluding high income)', \n",
    "                       'Euro area',\n",
    "                       'Europe & Central Asia',\n",
    "                       'Europe & Central Asia (IDA & IBRD countries)',\n",
    "                       'Europe & Central Asia (excluding high income)',\n",
    "                       'European Union',\n",
    "                       'Fragile and conflict affected situations',\n",
    "                       'Heavily indebted poor countries (HIPC)',\n",
    "                       'High income',\n",
    "                       'IBRD only',\n",
    "                       'IDA & IBRD total',\n",
    "                       'IDA blend',\n",
    "                       'IDA only',\n",
    "                       'IDA total',\n",
    "                       'Latin America & the Caribbean (IDA & IBRD countries)',\n",
    "                       'Latin America & Caribbean (excluding high income)',\n",
    "                       'Least developed countries: UN classification',\n",
    "                       'Low & middle income',\n",
    "                       'Low income',\n",
    "                       'Lower middle income',\n",
    "                       'Middle East & North Africa',\n",
    "                       'Middle East & North Africa (IDA & IBRD countries)',\n",
    "                       'Middle East & North Africa (excluding high income)',\n",
    "                       'Middle income',\n",
    "                       'Not classified',\n",
    "                       'OECD members',\n",
    "                       'Other small states',\n",
    "                       'Pacific island small states'\n",
    "                       'Post-demographic dividend',\n",
    "                       'Pre-demographic dividend',\n",
    "                       'Small states',\n",
    "                       'South Asia',\n",
    "                       'South Asia (IDA & IBRD)',\n",
    "                       'Sub-Saharan Africa (IDA & IBRD countries)',\n",
    "                       'Sub-Saharan Africa (excluding high income)',\n",
    "                       'Upper middle income',\n",
    "                       'World'\n",
    "                       ]\n",
    "\n",
    "# Drop the specified rows\n",
    "df_tax = df_tax.filter(~df_tax['country'].isin(countries_to_remove))\n",
    "\n",
    "# Remove rows with missing values in the 'country' column\n",
    "df_tax = df_tax.dropna(subset=['country'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $\\textbf{Unemployment - Processamento}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove columns Country Code and Indicator Code\n",
    "df_unemployment.drop(\"Country Code\", \"Indicator Code\", \"Indicator Name\")\n",
    "\n",
    "# Rename the column\n",
    "df_unemployment = df_unemployment.withColumnRenamed(\"Country Name\",\"country\")\n",
    "\n",
    "# Assuming df_unemployment is properly defined DataFrame\n",
    "df_unemployment = df_unemployment.select(\"country\", explode(array([\n",
    "    struct(lit(year).alias(\"year\"), col(str(year)).alias(\"unemployment\")) \n",
    "    for year in range(2000, 2023)\n",
    "])).alias(\"data\")).selectExpr(\"country\", \"data.year\", \"data.unemployment\")\n",
    "\n",
    "df_unemployment = df_unemployment.orderBy(\"country\")\n",
    "\n",
    "# Cast columns to their desired types\n",
    "df_unemployment = df_unemployment.withColumn(\"country\", col(\"country\").cast(\"string\"))\n",
    "df_unemployment = df_unemployment.withColumn(\"year\", col(\"year\").cast(\"int\"))\n",
    "df_unemployment = df_unemployment.withColumn(\"unemployment\", col(\"unemployment\").cast(\"double\"))\n",
    "\n",
    "# Filter data for years greater than 2010 and lower than 2024\n",
    "df_unemployment = df_unemployment.filter(df_unemployment[\"year\"] > 2010)\n",
    "df_unemployment = df_unemployment.filter(df_unemployment[\"year\"] < 2024)\n",
    "\n",
    "# Order by country and then by year\n",
    "df_unemployment = df_unemployment.orderBy(\"country\", \"year\")\n",
    "\n",
    "replacements = {\n",
    "    \"China, People's Republic of\": \"China\",\n",
    "    \"Gambia, The\": \"Gambia\",\n",
    "    \"Micronesia, Fed. States of\": \"Micronesia\",\n",
    "    \"South Sudan, Republic of\": \"South Sudan\",\n",
    "    \"Taiwan Province of China\": \"Taiwan\",\n",
    "    \"Türkiye, Republic of\": \"Türkiye\",\n",
    "    \"Caribbean small states\": \"Caribbean\",\n",
    "    \"Hong Kong SAR\": \"Hong Kong\",\n",
    "    \"Congo\": \"Democratic Republic of the Congo\",\n",
    "    \"Viet Nam\": \"Vietnam\"\n",
    "}\n",
    "\n",
    "# Rename columns based on replacements dictionary\n",
    "for old_value, new_value in replacements.items():\n",
    "    df_unemployment = df_unemployment.withColumnRenamed(old_value, new_value)\n",
    "\n",
    "# Columns to drop\n",
    "countries_to_remove = ['Africa Eastern and Southern', \n",
    "                       'Africa Western and Central', \n",
    "                       'Central Europe and the Baltics', \n",
    "                       'Early-demographic dividend', \n",
    "                       'East Asia & Pacific', \n",
    "                       'East Asia & Pacific (IDA & IBRD countries)', \n",
    "                       'East Asia & Pacific (excluding high income)', \n",
    "                       'Euro area',\n",
    "                       'Europe & Central Asia',\n",
    "                       'Europe & Central Asia (IDA & IBRD countries)',\n",
    "                       'Europe & Central Asia (excluding high income)',\n",
    "                       'European Union',\n",
    "                       'Fragile and conflict affected situations',\n",
    "                       'Heavily indebted poor countries (HIPC)',\n",
    "                       'High income',\n",
    "                       'IBRD only',\n",
    "                       'IDA & IBRD total',\n",
    "                       'IDA blend',\n",
    "                       'IDA only',\n",
    "                       'IDA total',\n",
    "                       'Latin America & the Caribbean (IDA & IBRD countries)',\n",
    "                       'Latin America & Caribbean (excluding high income)',\n",
    "                       'Least developed countries: UN classification',\n",
    "                       'Low & middle income',\n",
    "                       'Low income',\n",
    "                       'Lower middle income',\n",
    "                       'Middle East & North Africa',\n",
    "                       'Middle East & North Africa (IDA & IBRD countries)',\n",
    "                       'Middle East & North Africa (excluding high income)',\n",
    "                       'Middle income',\n",
    "                       'Not classified',\n",
    "                       'OECD members',\n",
    "                       'Other small states',\n",
    "                       'Pacific island small states'\n",
    "                       'Post-demographic dividend',\n",
    "                       'Pre-demographic dividend',\n",
    "                       'Small states',\n",
    "                       'South Asia',\n",
    "                       'South Asia (IDA & IBRD)',\n",
    "                       'Sub-Saharan Africa (IDA & IBRD countries)',\n",
    "                       'Sub-Saharan Africa (excluding high income)',\n",
    "                       'Upper middle income',\n",
    "                       'World',\n",
    "                       'ther small states'\n",
    "                       ]\n",
    "\n",
    "# Drop the specified rows\n",
    "df_unemployment = df_unemployment.filter(~df_unemployment['country'].isin(countries_to_remove))\n",
    "\n",
    "# Remove rows with missing values in the 'country' column\n",
    "df_unemployment = df_unemployment.dropna(subset=['country'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $\\textbf{Merge Files}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o1916.parquet.\n: java.lang.RuntimeException: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems\r\n\tat org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:735)\r\n\tat org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:270)\r\n\tat org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:286)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:978)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkOneDirWithMode(RawLocalFileSystem.java:660)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:700)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.mkdirs(ChecksumFileSystem.java:788)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.setupJob(FileOutputCommitter.java:356)\r\n\tat org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.setupJob(HadoopMapReduceCommitProtocol.scala:188)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:269)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)\r\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$executeCollect$1(AdaptiveSparkPlanExec.scala:390)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:418)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.executeCollect(AdaptiveSparkPlanExec.scala:390)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\r\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\r\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\r\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:859)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:388)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:361)\r\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:240)\r\n\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:792)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:567)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:830)\r\nCaused by: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems\r\n\tat org.apache.hadoop.util.Shell.fileNotFoundException(Shell.java:547)\r\n\tat org.apache.hadoop.util.Shell.getHadoopHomeDir(Shell.java:568)\r\n\tat org.apache.hadoop.util.Shell.getQualifiedBin(Shell.java:591)\r\n\tat org.apache.hadoop.util.Shell.<clinit>(Shell.java:688)\r\n\tat org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDurationHelper(Configuration.java:1907)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1867)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1840)\r\n\tat org.apache.hadoop.util.ShutdownHookManager.getShutdownTimeout(ShutdownHookManager.java:183)\r\n\tat org.apache.hadoop.util.ShutdownHookManager$HookEntry.<init>(ShutdownHookManager.java:207)\r\n\tat org.apache.hadoop.util.ShutdownHookManager.addShutdownHook(ShutdownHookManager.java:304)\r\n\tat org.apache.spark.util.SparkShutdownHookManager.install(ShutdownHookManager.scala:181)\r\n\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks$lzycompute(ShutdownHookManager.scala:50)\r\n\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks(ShutdownHookManager.scala:48)\r\n\tat org.apache.spark.util.ShutdownHookManager$.addShutdownHook(ShutdownHookManager.scala:153)\r\n\tat org.apache.spark.util.ShutdownHookManager$.<init>(ShutdownHookManager.scala:58)\r\n\tat org.apache.spark.util.ShutdownHookManager$.<clinit>(ShutdownHookManager.scala)\r\n\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:242)\r\n\tat org.apache.spark.util.SparkFileUtils.createTempDir(SparkFileUtils.scala:103)\r\n\tat org.apache.spark.util.SparkFileUtils.createTempDir$(SparkFileUtils.scala:102)\r\n\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:94)\r\n\tat org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment(SparkSubmit.scala:372)\r\n\tat org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:964)\r\n\tat org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:194)\r\n\tat org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:217)\r\n\tat org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:91)\r\n\tat org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1120)\r\n\tat org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1129)\r\n\tat org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)\r\nCaused by: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.\r\n\tat org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:467)\r\n\tat org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:438)\r\n\tat org.apache.hadoop.util.Shell.<clinit>(Shell.java:515)\r\n\t... 25 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 88\u001b[0m\n\u001b[0;32m     85\u001b[0m merged_df \u001b[38;5;241m=\u001b[39m merged_df\u001b[38;5;241m.\u001b[39morderBy(col(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcountry\u001b[39m\u001b[38;5;124m'\u001b[39m), col(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124myear\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m     87\u001b[0m \u001b[38;5;66;03m#Storing this dataframe in parquet\u001b[39;00m\n\u001b[1;32m---> 88\u001b[0m merged_df\u001b[38;5;241m.\u001b[39mwrite\u001b[38;5;241m.\u001b[39mmode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moverwrite\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mparquet(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFinalAllInOneParquet\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     89\u001b[0m spark\u001b[38;5;241m.\u001b[39mread\u001b[38;5;241m.\u001b[39mparquet(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFinalAllInOneParquet\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mshow()\n\u001b[0;32m     90\u001b[0m spark\u001b[38;5;241m.\u001b[39mstop()\n",
      "File \u001b[1;32mc:\\Users\\afons\\miniconda3\\envs\\daa1\\Lib\\site-packages\\pyspark\\sql\\readwriter.py:1721\u001b[0m, in \u001b[0;36mDataFrameWriter.parquet\u001b[1;34m(self, path, mode, partitionBy, compression)\u001b[0m\n\u001b[0;32m   1719\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpartitionBy(partitionBy)\n\u001b[0;32m   1720\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_opts(compression\u001b[38;5;241m=\u001b[39mcompression)\n\u001b[1;32m-> 1721\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jwrite\u001b[38;5;241m.\u001b[39mparquet(path)\n",
      "File \u001b[1;32mc:\\Users\\afons\\miniconda3\\envs\\daa1\\Lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[0;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\afons\\miniconda3\\envs\\daa1\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[0;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32mc:\\Users\\afons\\miniconda3\\envs\\daa1\\Lib\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o1916.parquet.\n: java.lang.RuntimeException: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems\r\n\tat org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:735)\r\n\tat org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:270)\r\n\tat org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:286)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:978)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkOneDirWithMode(RawLocalFileSystem.java:660)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:700)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.mkdirs(ChecksumFileSystem.java:788)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.setupJob(FileOutputCommitter.java:356)\r\n\tat org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.setupJob(HadoopMapReduceCommitProtocol.scala:188)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:269)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)\r\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$executeCollect$1(AdaptiveSparkPlanExec.scala:390)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:418)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.executeCollect(AdaptiveSparkPlanExec.scala:390)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\r\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\r\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\r\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:859)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:388)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:361)\r\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:240)\r\n\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:792)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:567)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:830)\r\nCaused by: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems\r\n\tat org.apache.hadoop.util.Shell.fileNotFoundException(Shell.java:547)\r\n\tat org.apache.hadoop.util.Shell.getHadoopHomeDir(Shell.java:568)\r\n\tat org.apache.hadoop.util.Shell.getQualifiedBin(Shell.java:591)\r\n\tat org.apache.hadoop.util.Shell.<clinit>(Shell.java:688)\r\n\tat org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDurationHelper(Configuration.java:1907)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1867)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1840)\r\n\tat org.apache.hadoop.util.ShutdownHookManager.getShutdownTimeout(ShutdownHookManager.java:183)\r\n\tat org.apache.hadoop.util.ShutdownHookManager$HookEntry.<init>(ShutdownHookManager.java:207)\r\n\tat org.apache.hadoop.util.ShutdownHookManager.addShutdownHook(ShutdownHookManager.java:304)\r\n\tat org.apache.spark.util.SparkShutdownHookManager.install(ShutdownHookManager.scala:181)\r\n\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks$lzycompute(ShutdownHookManager.scala:50)\r\n\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks(ShutdownHookManager.scala:48)\r\n\tat org.apache.spark.util.ShutdownHookManager$.addShutdownHook(ShutdownHookManager.scala:153)\r\n\tat org.apache.spark.util.ShutdownHookManager$.<init>(ShutdownHookManager.scala:58)\r\n\tat org.apache.spark.util.ShutdownHookManager$.<clinit>(ShutdownHookManager.scala)\r\n\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:242)\r\n\tat org.apache.spark.util.SparkFileUtils.createTempDir(SparkFileUtils.scala:103)\r\n\tat org.apache.spark.util.SparkFileUtils.createTempDir$(SparkFileUtils.scala:102)\r\n\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:94)\r\n\tat org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment(SparkSubmit.scala:372)\r\n\tat org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:964)\r\n\tat org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:194)\r\n\tat org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:217)\r\n\tat org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:91)\r\n\tat org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1120)\r\n\tat org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1129)\r\n\tat org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)\r\nCaused by: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.\r\n\tat org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:467)\r\n\tat org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:438)\r\n\tat org.apache.hadoop.util.Shell.<clinit>(Shell.java:515)\r\n\t... 25 more\r\n"
     ]
    }
   ],
   "source": [
    "dfs = [df_covid,df_gdp,df_inflation,df_migration,df_population,df_tax,df_unemployment]\n",
    "\n",
    "# Merge or outer join the DataFrames\n",
    "merged_df = dfs[0]\n",
    "\n",
    "for df in dfs[1:]:\n",
    "    merged_df = merged_df.join(df, on=[\"country\",\"year\"], how=\"outer\")\n",
    "\n",
    "replacements = {\n",
    "    \"Korea\": \"North Korea\",\n",
    "    \"Pacific island small states\": \"Pacific Islands\"\n",
    "}\n",
    "\n",
    "# Rename columns based on replacements dictionary\n",
    "for old_value, new_value in replacements.items():\n",
    "    merged_df = merged_df.withColumnRenamed(old_value, new_value)\n",
    "\n",
    "# Drop the specified columns\n",
    "columns_to_drop = ['t. Lucia',\n",
    "                   'ali', \n",
    "                   'alau',\n",
    "                   'Anguilla',\n",
    "                   'Bonaire Sint Eustatius and Saba',\n",
    "                   'Cook Islands',\n",
    "                   'American Samoa',\n",
    "                   'Arab World',\n",
    "                   'European Union',\n",
    "                   'Faeroe Islands',\n",
    "                   'Falkland Islands',\n",
    "                   'Faroe Islands',\n",
    "                   'French Guiana',\n",
    "                   'Guadeloupe',\n",
    "                   'Guernsey',\n",
    "                   'High income',\n",
    "                   'Isle of Man',\n",
    "                   'Jersey',\n",
    "                   'Kyrgyzstan',\n",
    "                   'Late-demographic dividend',\n",
    "                   'Latin America & Caribbean',\n",
    "                   'Low income',\n",
    "                   'Macao',              \n",
    "                   'Martinique',\n",
    "                   'Mayotte',\n",
    "                   'Montserrat',\n",
    "                   'Niue',\n",
    "                   'North America',\n",
    "                   'North Korea',\n",
    "                   'Northern Cyprus',\n",
    "                   'Northern Ireland',\n",
    "                   'Oceania',\n",
    "                   'Western Sahara',\n",
    "                   'Wales',\n",
    "                   'Wallis and Futuna',\n",
    "                   'Vatican',\n",
    "                   'Tokelau',\n",
    "                   'Sub-Saharan Africa',\n",
    "                   'Sub-Saharan Africa (Region) ',\n",
    "                   'St. Lucia',\n",
    "                   'St. Martin',\n",
    "                   'Sint Maarten',\n",
    "                   'Sint Maarten (Dutch part)',\n",
    "                   'Scotland',\n",
    "                   'Saint Martin',\n",
    "                   'Saint Lucia',\n",
    "                   'Saint Helena',\n",
    "                   'Reunion',\n",
    "                   'Post-demographic dividend',\n",
    "                   'Pitcairn',\n",
    "                   'Palestine',\n",
    "                   'England',\n",
    "                   'Saint Pierre and Miquelon',\n",
    "                   'Saint Barthelemy',\n",
    "                   'South America',\n",
    "                   'St. Martin (French part)'\n",
    "                   ]\n",
    "\n",
    "# Drop the specified rows\n",
    "merged_df = merged_df.filter(~df['country'].isin(columns_to_drop))\n",
    "\n",
    "# Rename \"no data\" to \"nan\"\n",
    "merged_df = merged_df.withColumn(\"country\", when(df[\"country\"] == \"no data\", \"nan\").otherwise(df[\"country\"]))\n",
    "merged_df = merged_df.withColumn(\"covid_cases\", when(col(\"covid_cases\").isNull(), 0).otherwise(col(\"covid_cases\")))\n",
    "\n",
    "# Sort the merged data by country and then by year\n",
    "merged_df = merged_df.orderBy(col('country'), col('year'))\n",
    "\n",
    "#Storing this dataframe in parquet\n",
    "merged_df.write.mode(\"overwrite\").parquet(\"FinalAllInOneParquet\")\n",
    "spark.read.parquet(\"FinalAllInOneParquet\").show()\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.308474779129028\n"
     ]
    }
   ],
   "source": [
    "fim = time.time()\n",
    "final = fim - inicio\n",
    "print(final)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DataScience",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
